{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import torch as tch\n",
    "import math\n",
    "# use proper seaborn styling\n",
    "import seaborn as sns\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see what device is available\n",
    "device = tch.device(\"cuda:0\" if tch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test-x.txt and test-y.txt are comma-seperated files of float/int values, corresponding to the board state and the policy respectively\n",
    "\n",
    "DATASET1 = \"datasets/dataset-4\"\n",
    "DATASET2 = \"datasets/dataset-3\"\n",
    "\n",
    "positions1       = np.loadtxt(f\"{DATASET1}/positions.csv\", delimiter=\",\")\n",
    "rollout_counts1  = np.loadtxt(f\"{DATASET1}/policy-target.csv\", delimiter=\",\")\n",
    "results1         = np.loadtxt(f\"{DATASET1}/value-target.csv\", delimiter=\",\")\n",
    "positions2      = np.loadtxt(f\"{DATASET2}/positions.csv\", delimiter=\",\")\n",
    "rollout_counts2 = np.loadtxt(f\"{DATASET2}/policy-target.csv\", delimiter=\",\")\n",
    "results2        = np.loadtxt(f\"{DATASET2}/value-target.csv\", delimiter=\",\")\n",
    "positions = np.concatenate((positions1, positions2))\n",
    "rollout_counts = np.concatenate((rollout_counts1, rollout_counts2))\n",
    "results = np.concatenate((results1, results2))\n",
    "\n",
    "print(f\"{len(positions)} datapoints loaded!\")\n",
    "assert len(positions) == len(rollout_counts)\n",
    "assert len(positions) == len(results)\n",
    "\n",
    "results[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the rollout counts\n",
    "r_sums = np.sum(rollout_counts, axis=1)\n",
    "rollout_counts /= r_sums[:, np.newaxis]\n",
    "\n",
    "print(f\"y has dims {rollout_counts.shape}\")\n",
    "print(f\"{rollout_counts[0]}\")\n",
    "\n",
    "# split the dataset into training and validation sets\n",
    "split = int(len(positions) * 0.9)\n",
    "x_train = positions[:split]\n",
    "y_train = rollout_counts[:split]\n",
    "z_train = results[:split]\n",
    "x_val = positions[split:]\n",
    "y_val = rollout_counts[split:]\n",
    "z_val = results[split:]\n",
    "\n",
    "# shuffle the datasets\n",
    "perm_train = np.random.permutation(len(x_train))\n",
    "x_train = x_train[perm_train]\n",
    "y_train = y_train[perm_train]\n",
    "z_train = z_train[perm_train]\n",
    "perm_val = np.random.permutation(len(x_val))\n",
    "x_val = x_val[perm_val]\n",
    "y_val = y_val[perm_val]\n",
    "z_val = z_val[perm_val]\n",
    "\n",
    "# convert to tensors\n",
    "x_train = tch.tensor(x_train, dtype=tch.float)\n",
    "y_train = tch.tensor(y_train, dtype=tch.float)\n",
    "z_train = tch.tensor(z_train, dtype=tch.float)\n",
    "print(f\"x_train has dims {x_train.shape}\")\n",
    "print(f\"y_train has dims {y_train.shape}\")\n",
    "print(f\"z_train has dims {z_train.shape}\")\n",
    "x_val = tch.tensor(x_val, dtype=tch.float)\n",
    "y_val = tch.tensor(y_val, dtype=tch.float)\n",
    "z_val = tch.tensor(z_val, dtype=tch.float)\n",
    "print(f\"x_val has dims {x_val.shape}\")\n",
    "print(f\"y_val has dims {y_val.shape}\")\n",
    "print(f\"z_val has dims {z_val.shape}\")\n",
    "\n",
    "# create a dataset class\n",
    "class GomokuDataset(tch.utils.data.Dataset):\n",
    "    def __init__(self, pos, policy, value):\n",
    "        self.x = pos\n",
    "        self.y = policy\n",
    "        self.z = value\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx], self.z[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "# create dataloaders\n",
    "train_dataset = GomokuDataset(x_train, y_train, z_train)\n",
    "val_dataset = GomokuDataset(x_val, y_val, z_val)\n",
    "train_loader = tch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = tch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_shaped = x_train.reshape(-1, 2, 9, 9)\n",
    "y_shaped = y_train.reshape(-1, 9, 9)\n",
    "\n",
    "# show some sample data\n",
    "fig, axs = plt.subplots(5, 4, figsize=(12, 10))\n",
    "for i in range(5):\n",
    "    # left plot is the board state\n",
    "    board = x_shaped[i, 0] + x_shaped[i, 1] / 2\n",
    "    axs[i, 0].imshow(board, cmap=\"gray\")\n",
    "    # right plot is the policy\n",
    "    policy = y_shaped[i]\n",
    "    axs[i, 1].imshow(policy, cmap=\"inferno\")\n",
    "\n",
    "    # plot the nonzero board values\n",
    "    nonzero_board = board != 0\n",
    "    axs[i, 2].imshow(nonzero_board, vmin=0, vmax=1)\n",
    "\n",
    "    # plot the zero policy values\n",
    "    zero_policy = policy == 0\n",
    "    axs[i, 3].imshow(zero_policy, vmin=0, vmax=1)\n",
    "\n",
    "# remove the gridlines\n",
    "for ax in axs.flatten():\n",
    "    ax.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an extremely simple model\n",
    "from matplotlib.pylab import f\n",
    "\n",
    "BOARD_SIDE_LEN = 9\n",
    "SQUARES = BOARD_SIDE_LEN * BOARD_SIDE_LEN\n",
    "\n",
    "# define a convolutional model\n",
    "# this is ever so slightly more complicated than the previous model\n",
    "# as we need to reshape the input to be 4-dimensional\n",
    "class ConvPolicyModel(tch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu    = tch.nn.ReLU()\n",
    "        self.sigmoid = tch.nn.Sigmoid()\n",
    "        # five layers of 3x3 convs mean that information can travel at most five squares away.\n",
    "        # this is maybe fine idk\n",
    "        self.conv1   = tch.nn.Conv2d(2, 64, 3, padding=1) # 2x9x9 -> 64x9x9\n",
    "        self.conv2   = tch.nn.Conv2d(64, 64, 3, padding=1) # 64x9x9 -> 64x9x9\n",
    "        self.conv3   = tch.nn.Conv2d(64, 32, 3, padding=1) # 64x9x9 -> 32x9x9\n",
    "        self.conv4   = tch.nn.Conv2d(32, 16, 3, padding=1) # 32x9x9 -> 16x9x9\n",
    "        self.conv5   = tch.nn.Conv2d(16, 2, 3, padding=1) # 8x9x9 -> 2x9x9\n",
    "        self.policy1 = tch.nn.Linear(2 * SQUARES, 2 * SQUARES)\n",
    "        self.policy2 = tch.nn.Linear(2 * SQUARES, SQUARES)\n",
    "        self.value1  = tch.nn.Linear(2 * SQUARES, 2 * SQUARES)\n",
    "        self.value2  = tch.nn.Linear(2 * SQUARES, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 2, BOARD_SIDE_LEN, BOARD_SIDE_LEN)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu(x)\n",
    "        # flatten\n",
    "        latent = x.view(-1, 2 * SQUARES)\n",
    "        x = self.policy1(latent)\n",
    "        x = self.relu(x)\n",
    "        policy_logits = self.policy2(x)\n",
    "        x = self.value1(latent)\n",
    "        x = self.relu(x)\n",
    "        x = self.value2(x)\n",
    "        value = self.sigmoid(x)\n",
    "        return policy_logits, value\n",
    "\n",
    "class SimpleNet(tch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sigmoid = tch.nn.Sigmoid()\n",
    "        self.policy  = tch.nn.Linear(2 * SQUARES, SQUARES)\n",
    "        self.value   = tch.nn.Linear(2 * SQUARES, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        policy_logits = self.policy(x)\n",
    "        value = self.sigmoid(self.value(x))\n",
    "        return policy_logits, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model and optimizer\n",
    "model = ConvPolicyModel()\n",
    "optimizer = tch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.05)\n",
    "\n",
    "# create a loss function to match the probability distribution\n",
    "def loss_fn(prediction_logits, target_distribution):\n",
    "    return tch.nn.functional.binary_cross_entropy(tch.nn.functional.softmax(prediction_logits, dim=1), target_distribution)\n",
    "\n",
    "# create a function for masking off illegal moves\n",
    "def mask_illegal_moves(model_prediction, board):\n",
    "    # return model_prediction\n",
    "    # model_prediction is an 81-element vector of probabilities\n",
    "    # board is a 81 * 2 = 162-element vector of occupancies\n",
    "    # we need to set all illegal moves to 0,\n",
    "    # and then renormalize the probabilities\n",
    "    # so that they sum to 1 again\n",
    "    # first, get a mask of all illegal moves\n",
    "    illegal_moves = board[:, :81] + board[:, 81:]\n",
    "    # now set all illegal moves to 0\n",
    "    model_prediction = tch.where(illegal_moves != 0, tch.zeros_like(model_prediction), model_prediction)\n",
    "\n",
    "    return model_prediction\n",
    "\n",
    "POLICY_SOFTMAX_TEMP = 1.3\n",
    "def clean_model_prediction(model_prediction, board):\n",
    "    model_prediction = mask_illegal_moves(model_prediction, board)\n",
    "    model_prediction = model_prediction * POLICY_SOFTMAX_TEMP\n",
    "    return model_prediction\n",
    "\n",
    "# create a training loop\n",
    "def train(model, optimizer, train_loader, val_loader, epochs=20):\n",
    "    model = model.to(device)\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        model.train()\n",
    "        for batch_idx, (board_state, search_policy, game_outcome) in enumerate(train_loader):\n",
    "            board_state = board_state.to(device)\n",
    "            search_policy = search_policy.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            raw_policy, value = model(board_state)\n",
    "            masked_policy = clean_model_prediction(raw_policy, board_state)\n",
    "            policy_loss = loss_fn(masked_policy, search_policy)\n",
    "            value_loss = tch.nn.functional.mse_loss(value.view(-1), game_outcome)\n",
    "            loss = policy_loss * 10 + value_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            policy_loss_value = policy_loss.item()\n",
    "            value_loss_value = value_loss.item()\n",
    "            loss_value = loss.item()\n",
    "            total_batch_idx = epoch * len(train_loader) + batch_idx\n",
    "            losses.append((total_batch_idx, loss_value, policy_loss_value, value_loss_value))\n",
    "            if batch_idx % 256 == 0:\n",
    "                print(f\"Training batch {batch_idx}/{len(train_loader)}: loss {loss_value}, policy loss {policy_loss_value}, value loss {value_loss_value}\")\n",
    "        val_policy_loss = 0.0\n",
    "        val_value_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        with tch.no_grad():\n",
    "            for batch_idx, (board_state, search_policy, game_outcome) in enumerate(val_loader):\n",
    "                board_state = board_state.to(device)\n",
    "                search_policy = search_policy.to(device)\n",
    "                raw_policy, value = model(board_state)\n",
    "                masked_policy = clean_model_prediction(raw_policy, board_state)\n",
    "                policy_loss = loss_fn(masked_policy, search_policy)\n",
    "                value_loss = tch.nn.functional.mse_loss(value.view(-1), game_outcome)\n",
    "                val_policy_loss += policy_loss.item()\n",
    "                val_value_loss += value_loss.item()\n",
    "                val_loss += (policy_loss + value_loss).item()\n",
    "            val_policy_loss /= len(val_loader)\n",
    "            val_value_loss /= len(val_loader)\n",
    "            val_loss /= len(val_loader)\n",
    "            total_batch_idx = (epoch + 1) * len(train_loader)\n",
    "            val_losses.append((total_batch_idx, val_loss, val_policy_loss, val_value_loss))\n",
    "            print(f\"Validation loss {val_loss}, policy loss {val_policy_loss}, value loss {val_value_loss}\")\n",
    "    return losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "# print(f\"Training on device {tch.cuda.get_device_name(0)}\")\n",
    "loss_trace, val_loss_trace = train(model, optimizer, train_loader, val_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss trace and validation loss trace\n",
    "loss_trace = np.array(loss_trace)\n",
    "val_loss_trace = np.array(val_loss_trace)\n",
    "plt.plot(loss_trace[:, 0], loss_trace[:, 1])\n",
    "plt.plot(val_loss_trace[:, 0], val_loss_trace[:, 1])\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Training loss\", \"Validation loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for policy and value losses particularly\n",
    "plt.plot(loss_trace[:, 0], loss_trace[:, 2])\n",
    "plt.plot(val_loss_trace[:, 0], val_loss_trace[:, 2])\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Policy Loss\")\n",
    "plt.legend([\"Training policy loss\", \"Validation policy loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_trace[:, 0], loss_trace[:, 3])\n",
    "plt.plot(val_loss_trace[:, 0], val_loss_trace[:, 3])\n",
    "# add the training loss trace smoothed\n",
    "smoothed = np.convolve(loss_trace[:, 3], np.ones(1000)/1000, mode=\"valid\")\n",
    "plt.plot(loss_trace[500:-499, 0], smoothed)\n",
    "# add dots to the validation loss trace\n",
    "sc = plt.scatter(val_loss_trace[:, 0], val_loss_trace[:, 3], c=\"orange\", s=10)\n",
    "# bring the dots to the front\n",
    "sc.set_zorder(10)\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Value Loss\")\n",
    "# put the legend in the middle right\n",
    "plt.legend([\"Training value loss\", \"Validation value loss\", \"Smoothed training value loss\"], loc=\"best\")\n",
    "plt.title(\"Value Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "tch.save(model.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the model's predictions\n",
    "model.eval()\n",
    "\n",
    "# put the model on the cpu\n",
    "model = model.cpu()\n",
    "\n",
    "# get N random items from the validation set\n",
    "N_SAMPLES = 100\n",
    "rand_idx = np.random.randint(len(val_dataset), size=N_SAMPLES)\n",
    "x_sample = x_val[rand_idx]\n",
    "y_sample = y_val[rand_idx]\n",
    "y_pred_raw, value = model(x_sample)\n",
    "y_pred = clean_model_prediction(y_pred_raw, x_sample)\n",
    "# apply softmax to get a probability distribution\n",
    "y_pred = tch.nn.functional.softmax(y_pred, dim=1)\n",
    "\n",
    "x_sample_re = x_sample.reshape(-1, 2, 9, 9)\n",
    "y_sample_re = y_sample.reshape(-1, 9, 9)\n",
    "y_pred_re = y_pred.detach().numpy().reshape(-1, 9, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "# the first column is the board state\n",
    "# the second column is the search policy\n",
    "# the third column is the neural network policy\n",
    "fig, axs = plt.subplots(N_SAMPLES, 3, figsize=(7, int(math.ceil(13 / 5 * N_SAMPLES))))\n",
    "# label the columns\n",
    "axs[0, 0].set_title(\"Board state\")\n",
    "axs[0, 1].set_title(\"Search policy\")\n",
    "axs[0, 2].set_title(\"Neural network policy\")\n",
    "for i in range(N_SAMPLES):\n",
    "    board_one = x_sample_re[i][0]\n",
    "    board_two = x_sample_re[i][1]\n",
    "    search = y_sample_re[i]\n",
    "    nn = y_pred_re[i]\n",
    "    # pieces_on_first_board = board_one.flatten().sum()\n",
    "    # pieces_on_second_board = board_two.flatten().sum()\n",
    "    # x_to_move = pieces_on_first_board != pieces_on_second_board\n",
    "    # if not x_to_move:\n",
    "    #     board_one, board_two = board_two, board_one\n",
    "    board = np.stack([board_one, np.zeros((9, 9)), board_two], axis=2) / 1.5\n",
    "    search_policy_board = np.stack([search, search, search], axis=2)\n",
    "    nn_policy_board = np.stack([nn, nn, nn], axis=2)\n",
    "\n",
    "    # renormalise the policies so that the max prediction is 1.0\n",
    "    search_policy_board *= 1.0 / search_policy_board.max()\n",
    "    nn_policy_board *= 1.0 / nn_policy_board.max()\n",
    "\n",
    "    search_policy_board += board\n",
    "    nn_policy_board += board\n",
    "    axs[i, 0].imshow(board, vmin=0, vmax=255, cmap=\"inferno\")\n",
    "    axs[i, 1].imshow(search_policy_board, vmin=0, vmax=255)\n",
    "    axs[i, 2].imshow(nn_policy_board, vmin=0, vmax=255)\n",
    "\n",
    "# set gridlines to 1x1\n",
    "for ax in axs.flatten():\n",
    "    ax.set_xticks(np.arange(-.5, 9, 1))\n",
    "    ax.set_yticks(np.arange(-.5, 9, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.grid(True, color=\"grey\", linewidth=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export model to ONNX\n",
    "onnx_model_path = \"model.onnx\"\n",
    "\n",
    "# create a dummy input\n",
    "dummy_input = tch.randn(1, 162)\n",
    "\n",
    "# export the model\n",
    "batch_axis = {0: \"batch_size\"}\n",
    "\n",
    "tch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_model_path,\n",
    "    verbose=True,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"policy\", \"value\"],\n",
    "    dynamic_axes={\"input\": batch_axis, \"policy\": batch_axis, \"value\": batch_axis},\n",
    "    opset_version=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the model with onnxruntime\n",
    "common_input_data = x_sample\n",
    "pytorch_net_output = model(common_input_data)\n",
    "pytorch_policy, pytorch_value = pytorch_net_output\n",
    "print(f\"PyTorch policy has shape {pytorch_policy.shape}\")\n",
    "print(f\"PyTorch value has shape {pytorch_value.shape}\")\n",
    "print(f\"x_sample has shape {common_input_data.shape}\")\n",
    "for i in range(len(common_input_data)):\n",
    "    pytorch_policy[i] = clean_model_prediction(pytorch_policy[i].reshape(1, 81), common_input_data[i].reshape(1, 162))\n",
    "pytorch_policy = tch.nn.functional.softmax(pytorch_policy, dim=1)\n",
    "pytorch_policy = pytorch_policy.detach().numpy()\n",
    "pytorch_value = pytorch_value.detach().numpy()\n",
    "import onnxruntime as ort\n",
    "ort_session = ort.InferenceSession(onnx_model_path)\n",
    "onnx_net_output = []\n",
    "for i in range(len(common_input_data)):\n",
    "    input_thing = {'onnx::Reshape_0': common_input_data[i].numpy().reshape(1, 162)}\n",
    "    ort_session_out = ort_session.run(None, input_thing)\n",
    "    policy = ort_session_out[0]\n",
    "    np_policy = policy.reshape(1, 81)\n",
    "    tensor_policy = tch.tensor(np_policy)\n",
    "    policy = clean_model_prediction(tensor_policy, common_input_data[i].reshape(1, 162))\n",
    "    policy = tch.nn.functional.softmax(policy, dim=1)\n",
    "    onnx_net_output.append(policy)\n",
    "onnx_net_output = np.squeeze(np.array(onnx_net_output), axis=1)\n",
    "\n",
    "print(f\"ONNX policy has shape {onnx_net_output.shape}\")\n",
    "\n",
    "onnx_policy = onnx_net_output\n",
    "\n",
    "# compare the outputs\n",
    "assert pytorch_policy.shape == onnx_policy.shape\n",
    "assert np.allclose(pytorch_policy, onnx_policy, rtol=1e-03, atol=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_policy_merge(A, B):\n",
    "    # Reshape A and B tensors\n",
    "    A_reshaped = A.view(A.size(0), A.size(1), 1, -1)  # Shape: [BATCH, N, 1, M]\n",
    "    B_reshaped = B.view(B.size(0), 1, B.size(1), -1)  # Shape: [BATCH, 1, N, M]\n",
    "\n",
    "    # Perform element-wise multiplication\n",
    "    C = tch.matmul(A_reshaped, B_reshaped.transpose(2, 3))  # Shape: [BATCH, N, N, M]\n",
    "\n",
    "    # Sum along the last dimension to get dot products\n",
    "    C = tch.sum(C, dim=-1)  # Shape: [BATCH, N, N]\n",
    "\n",
    "    return C\n",
    "\n",
    "def compute_dot_product(A, B):\n",
    "    batch_size, n, m = A.size()\n",
    "\n",
    "    # Reshape A to [batch_size, n, 1, m]\n",
    "    A = A.unsqueeze(2)\n",
    "\n",
    "    # Reshape B to [batch_size, 1, m, n]\n",
    "    B = B.transpose(1, 2).unsqueeze(1)\n",
    "\n",
    "    # Compute the dot product\n",
    "    C = tch.matmul(A, B).squeeze(2)\n",
    "\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = tch.randn(2, 3, 8)\n",
    "tgt = tch.randn(2, 3, 8)\n",
    "res = compute_dot_product(src, tgt)\n",
    "print(src.shape)\n",
    "print(tgt.shape)\n",
    "print(res.shape)\n",
    "print(src)\n",
    "print(tgt)\n",
    "print(res)\n",
    "\n",
    "# assert that the operation is correct\n",
    "for batch in range(len(src)):\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            result = res[batch, i, j]\n",
    "            src_vec = src[batch, i]\n",
    "            tgt_vec = tgt[batch, j]\n",
    "            expected_result = tch.dot(src_vec, tgt_vec)\n",
    "            are_close = tch.isclose(result, expected_result, rtol=1e-03, atol=1e-05)\n",
    "            assert are_close, f\"Expected {expected_result}, got {result}, at batch {batch}, i {i}, j {j}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
